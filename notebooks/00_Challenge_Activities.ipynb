{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challange Activities\n",
    "\n",
    "This notebook in a markdown formart describe all the steps and answers the proposed challange activities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - What steps would you take to solve this problem?\n",
    "\n",
    "Each of the steps from 1 to 5 presented below will be applied and detailed in a corresponding *Jupyter Notebook* with the respective number.\n",
    "\n",
    "The steps 6 to 8 are embedded in the notebook number 5.\n",
    "\n",
    "**Step 1 - Data Understanding and Cleaning:**\n",
    "- Load the datasets and understand their structure and contents.\n",
    "    - This part of Step 1 addresses the multiple-choice questions of the challenge.\n",
    "- Handle missing values denoted by na.\n",
    "- Generate new cleaned data files for easy access and use.\n",
    "\n",
    "**Step 2 - Exploratory Data Analysis (EDA):**\n",
    "\n",
    "- Perform descriptive statistics to understand the distribution of data.\n",
    "- Visualize data to identify patterns, correlations, and anomalies.\n",
    "\n",
    "**Step 3 - Feature Engineering:**\n",
    "\n",
    "- Normalize or standardize numerical features if necessary.\n",
    "- Apply SMOTE (Synthetic Minority Over-sampling Technique) to balance the class distribution by generating synthetic samples for the minority class.\n",
    "\n",
    "**Step 4 - Dimensionality Reduction:**\n",
    "\n",
    "- Use PCA (Principal Component Analysis) and Factor Analysis as techniques to reduce the number of features while retaining most of the variance.\n",
    "- Use feature selection methods to identify the most important features.\n",
    "    - Recursive Feature Elimination (RFE)\n",
    "    - Feature Importance from Tree-based Models (e.g., Random Forest)\n",
    "\n",
    "**Step 5 - Model Training and Evaluation:**\n",
    "\n",
    "- Split the data into training and test sets, ensuring the test set reflects the present year's data.\n",
    "- Train multiple predictive models, such as:\n",
    "    - KNN Classifier\n",
    "    - Random Forest\n",
    "    - Neural Networks\n",
    "- Use techniques like cross-validation to tune hyperparameters and avoid overfitting.\n",
    "- Evaluate the models using relevant metrics.\n",
    "- Select the best-performing model based on evaluation results.\n",
    "\n",
    "**Step 6 - Interpretability and Insights:**\n",
    "\n",
    "- Use techniques like SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations) to interpret the model's predictions.\n",
    "- Identify the main factors contributing to air system failures.\n",
    "\n",
    "**Step 7 - Business Metrics and Recommendations:**\n",
    "\n",
    "- Translate technical metrics to business metrics to demonstrate cost savings.\n",
    "- Provide actionable recommendations based on model insights.\n",
    "\n",
    "**Step 8 - Presentation:**\n",
    "\n",
    "- Prepare a comprehensive report and presentation for the executive board, highlighting the potential cost savings and the main factors leading to air system failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Which technical data science metric would you use to solve this challenge?\n",
    "\n",
    "- **Accuracy:** To measure the proportion of correctly predicted maintenance needs.\n",
    "- **Precision and Recall:** To balance the trade-off between identifying true positives and avoiding false negatives.\n",
    "- **F1 Score:** To provide a single metric that balances precision and recall. (Specially the Macro Avg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Which business metric would you use to solve the challenge?\n",
    "\n",
    "- **Cost Savings:** The primary business metric will be the reduction in maintenance costs for the air system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - How do technical metrics relate to the business metrics?\n",
    "\n",
    "- Reducing false negatives (trucks with defects not identified) translates directly to cost savings by avoiding expensive corrective maintenance.\n",
    "- Improving precision reduces unnecessary preventive maintenance costs, ensuring resources are allocated efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - What types of analyzes would you like to perform on the customer database?\n",
    "\n",
    "- If we had access to the specific data and time of the data collected:\n",
    "    - Trend analysis of maintenance costs and occurences.\n",
    "    - Failure rate analysis over time.\n",
    "- Correlation analysis between features and air system failures.\n",
    "- Cost impact analysis of false negatives and false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - What techniques would you use to reduce the dimensionality of the problem? \n",
    "\n",
    "- Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - What techniques would you use to select variables for your predictive model?\n",
    "\n",
    "- Recursive Feature Elimination (RFE)\n",
    "- Feature Importance from Tree-based Models (e.g., Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 - What predictive models would you use or test for this problem? \n",
    "\n",
    "- **KNN Classifier:** Simple and effective for capturing complex relationships by considering the distance between data points.\n",
    "- **Random Forest:** For capturing non-linear relationships and feature importance.\n",
    "- **Gradient Boosting Machines (GBM):** For high-performance prediction. (I am running in a MacOS Env., so I have some limitations to install and run XGBoost, for example, in the available time).\n",
    "- **Neural Networks:** For capturing complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 - How would you rate which of the trained models is the best?\n",
    "\n",
    "- Use of technical metrics:\n",
    "    - **Macro Average F1-Score**\n",
    "    - **Confusion Matrix:** To visualize true positives, false positives, true negatives, and false negatives.\n",
    "- Use of business metrics:\n",
    "    - **Cost Analysis:** To quantify the cost savings achieved by the model in reducing maintenance expenses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 - How would you explain the result of your model? Is it possible to know which variables are most important?\n",
    "\n",
    "It's really important to know which variables are most important, both from a technical side to develop better models and from a business side to correctly alert the company about the true root causes.\n",
    "\n",
    "- **Feature Importance:** Use model-specific methods to identify which features contribute most to predictions.\n",
    "- **SHAP Values:** Explain individual predictions by showing the impact of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 - How would you assess the financial impact of the proposed model?\n",
    "\n",
    "- **Calculate Savings:** Estimate cost savings by comparing predicted maintenance needs versus actual maintenance costs.\n",
    "- **Scenario Analysis:** Model different scenarios to see potential cost impacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 - What techniques would you use to perform the hyperparameter optimization of the chosen model?\n",
    "\n",
    "- **Grid Search:** Exhaustive search over a specified parameter grid.\n",
    "- **Random Search:** Randomly samples parameter combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13 - What risks or precautions would you present to the customer before putting this model into production?\n",
    "\n",
    "- **Model Overfitting:** Ensure the model generalizes well to unseen data.\n",
    "- **Data Quality:** Monitor and address potential issues with missing or erroneous data.\n",
    "- **Interpretability:** Ensure the model's decisions can be understood by stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14 - If your predictive model is approved, how would you put it into production?\n",
    "\n",
    "This is one of the most critical steps of the project, for this reason I would use the follow steps:\n",
    "\n",
    "**Integration:**\n",
    "\n",
    "- **Model Serving API:** To integrate the model with existing systems, we can build a Model Serving API in Python. A backbone of this structure is available on my GitHub repository for Flask and FastAPI. See the links below:\n",
    "\n",
    "    - https://github.com/michelhilg/model-serving-flaskAPI\n",
    "    - https://github.com/michelhilg/model-serving-fastAPI\n",
    "\n",
    "    This backbone provides a foundation for serving the model and can be customized to meet the specific needs.\n",
    "\n",
    "**Deployment:**\n",
    "\n",
    "- **Docker Containers:** For deployment, we can use Docker containers to encapsulate the model and its dependencies. Docker containers ensure that the model runs consistently across different environments, making it easier to manage and scale.\n",
    "\n",
    "- **Production Environment:** Deploy the Docker containers inside the client's production environment. This approach helps in maintaining compatibility with the existing infrastructure and facilitates seamless integration.\n",
    "\n",
    "**Documentation:**\n",
    "\n",
    "- **Model Documentation:** Document the model's assumptions, features, and usage comprehensively. This documentation will be crucial for ensuring that stakeholders understand how the model works and can effectively use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15 - If the model is in production, how would you monitor it?\n",
    "\n",
    "- **Performance Tracking:** Regularly check the model's accuracy and other metrics.\n",
    "- **Feedback Loop:** Collect feedback from users and adjust the model as needed.\n",
    "\n",
    "For the Performance Tracking, we can some tools like:\n",
    "\n",
    "- **Prometheus & Grafana:**\n",
    "\n",
    "    - **Prometheus:** Collects and stores metrics from your application. It can monitor model performance metrics and alert you based on thresholds.\n",
    "\n",
    "    - **Grafana:** Visualizes the metrics collected by Prometheus. It allows you to create dashboards and monitor the performance of your model in real time.\n",
    "\n",
    "    - Both os them runs also inside docker containers, which can help and the deployment section.\n",
    "\n",
    "- **Custom Tracking:**\n",
    "\n",
    "    - Build a simple web interface that runs directly within the client's production environment. This approach provides a straightforward and effective solution for monitoring the model, allowing for real-time tracking of performance metrics and easy access to monitoring data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16 - If the model is in production, how would you know when to retrain it?\n",
    "\n",
    "**1. Performance Drift:**\n",
    "\n",
    "- **Performance Metrics Monitoring:** Regularly track performance metrics such as accuracy, precision, recall, or F1 score. Retrain the model if these metrics show significant degradation over time.\n",
    "\n",
    "- **Drift Detection Tools:**\n",
    "\n",
    "    - **Evidently AI:** Provides tools to detect and visualize performance drift, including data and concept drift.\n",
    "    - **Deep Checks:** More focused on LLMs models, but with a nice set of Python tools for comprehensive model monitoring and validation, specially in terms of data drift as well.\n",
    "    - **Custom Alerts:** Set up custom alerts in monitoring tools like Prometheus or Datadog to notify you when performance metrics fall below predefined thresholds.\n",
    "\n",
    "    Now that we already know that the model should be retrained, we can apply some periodic retraining methods:\n",
    "\n",
    "- **Periodic Retraining:**\n",
    "\n",
    "    - **MLflow:** Helps manage the machine learning lifecycle, including scheduling regular retraining based on new data availability."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
